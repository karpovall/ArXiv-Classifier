{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import transformers\n",
    "import json\n",
    "import pandas as pd\n",
    "import arxiv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning\n",
    "import warnings\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_types = ['cs', 'econ', 'eess', 'math', 'astro-ph', 'cond-mat', 'gr-qc', 'hep-ex', 'hep-lat', 'hep-ph', 'hep-th', 'math-ph', 'nlin', 'nucl-ex', 'nucl-th', 'physics', 'quant-ph', 'q-bio', 'q-fin', 'stat']\n",
    "term_dict = {}\n",
    "for term_type in term_types:\n",
    "    term_dict[term_type] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_terms(input_series):\n",
    "    result = []\n",
    "\n",
    "    for item in input_series:\n",
    "        clean_string = item.replace(\"\", \"\").replace(\"\", \"\").replace(\"{\", \"\").replace(\"}\", \"\")\n",
    "        clean_string = clean_string.replace(\"'\", \"\").replace('\"', \"\").replace(\":\", \"\").replace(\", \", \",\")\n",
    "        \n",
    "        terms = clean_string.split('term')\n",
    "        \n",
    "        terms_list = []\n",
    "        for term in terms[1:]:\n",
    "            term = term.split(',')[0].strip()\n",
    "            if term:\n",
    "                for term_type in term_types:\n",
    "                    if term_type in term:\n",
    "                        term_dict[term_type] += 1\n",
    "                        terms_list.append(term)\n",
    "                        break\n",
    "        if terms_list == []:\n",
    "            result.append(None)\n",
    "        else:\n",
    "            result.append(terms_list[0])\n",
    "    return result\n",
    "\n",
    "def make_text(title, summary):\n",
    "    result = []\n",
    "    for t, s in zip(title, summary):\n",
    "        result.append(f'Title: {t}\\nSummary: {s}')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('arxivData.json')\n",
    "df = df[['summary', 'tag', 'title']]\n",
    "df['tag'] = parse_terms(df['tag'])\n",
    "df = df.groupby('tag').filter(lambda x: len(x) > 1)\n",
    "df['text'] = make_text(df['title'], df['summary'])\n",
    "df = df[['text', 'tag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Title: Dual Recurrent Attention Units for Visu...</td>\n",
       "      <td>cs.AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Title: Sequential Short-Text Classification wi...</td>\n",
       "      <td>cs.CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Title: Multiresolution Recurrent Neural Networ...</td>\n",
       "      <td>cs.CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Title: Learning what to share between loosely ...</td>\n",
       "      <td>stat.ML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Title: A Deep Reinforcement Learning Chatbot\\n...</td>\n",
       "      <td>cs.CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40995</th>\n",
       "      <td>Title: Nearly Tight Bounds on $\\ell_1$ Approxi...</td>\n",
       "      <td>cs.LG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40996</th>\n",
       "      <td>Title: Concurrent bandits and cognitive radio ...</td>\n",
       "      <td>cs.LG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40997</th>\n",
       "      <td>Title: A Comparison of Clustering and Missing ...</td>\n",
       "      <td>math.NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40998</th>\n",
       "      <td>Title: Applying machine learning to the proble...</td>\n",
       "      <td>cs.SC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40999</th>\n",
       "      <td>Title: A Multi Level Data Fusion Approach for ...</td>\n",
       "      <td>cs.SD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40983 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text      tag\n",
       "0      Title: Dual Recurrent Attention Units for Visu...    cs.AI\n",
       "1      Title: Sequential Short-Text Classification wi...    cs.CL\n",
       "2      Title: Multiresolution Recurrent Neural Networ...    cs.CL\n",
       "3      Title: Learning what to share between loosely ...  stat.ML\n",
       "4      Title: A Deep Reinforcement Learning Chatbot\\n...    cs.CL\n",
       "...                                                  ...      ...\n",
       "40995  Title: Nearly Tight Bounds on $\\ell_1$ Approxi...    cs.LG\n",
       "40996  Title: Concurrent bandits and cognitive radio ...    cs.LG\n",
       "40997  Title: A Comparison of Clustering and Missing ...  math.NA\n",
       "40998  Title: Applying machine learning to the proble...    cs.SC\n",
       "40999  Title: A Multi Level Data Fusion Approach for ...    cs.SD\n",
       "\n",
       "[40983 rows x 2 columns]"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.3, random_state=42, stratify=df['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label: i for i, label in enumerate(df['tag'].unique())}\n",
    "inv_label_map = {v: k for k, v in label_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c06e003c1214796ac9ff919a7a65a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/20.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5943b0f9f7624b15b982ab2a90dda713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.13M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6481a64841194920a15902f1aa190c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2dadbc6cec461cb6fef3f96ac04e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fbeccee7db7463e945da53f89fa24a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/599M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'answerdotai/ModernBERT-base'\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label_map),\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=128):\n",
    "        self.texts = df['text'].tolist()\n",
    "        self.labels = df['tag'].map(label_map).tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'labels': torch.tensor(label)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "train_dataset = TextDataset(train, tokenizer)\n",
    "test_dataset = TextDataset(test, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "epochs = 10\n",
    "total_steps = len(train_loader) * epochs\n",
    "\n",
    "scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate(model, test_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            correct += (predictions == batch['labels']).sum().item()\n",
    "    \n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 1.4090\n",
      "Test Loss: 1.2254\n",
      "Test Accuracy: 0.6350\n",
      "----------------------------------------\n",
      "Epoch 2/10\n",
      "Train Loss: 1.0587\n",
      "Test Loss: 1.1388\n",
      "Test Accuracy: 0.6535\n",
      "----------------------------------------\n",
      "Epoch 3/10\n",
      "Train Loss: 0.8021\n",
      "Test Loss: 1.1644\n",
      "Test Accuracy: 0.6576\n",
      "----------------------------------------\n",
      "Epoch 4/10\n",
      "Train Loss: 0.4809\n",
      "Test Loss: 1.3751\n",
      "Test Accuracy: 0.6275\n",
      "----------------------------------------\n",
      "Epoch 5/10\n",
      "Train Loss: 0.1847\n",
      "Test Loss: 1.5100\n",
      "Test Accuracy: 0.6415\n",
      "----------------------------------------\n",
      "Epoch 6/10\n",
      "Train Loss: 0.0450\n",
      "Test Loss: 1.6904\n",
      "Test Accuracy: 0.6373\n",
      "----------------------------------------\n",
      "Epoch 7/10\n",
      "Train Loss: 0.0127\n",
      "Test Loss: 1.8795\n",
      "Test Accuracy: 0.6472\n",
      "----------------------------------------\n",
      "Epoch 8/10\n",
      "Train Loss: 0.0050\n",
      "Test Loss: 1.9972\n",
      "Test Accuracy: 0.6429\n",
      "----------------------------------------\n",
      "Epoch 9/10\n",
      "Train Loss: 0.0035\n",
      "Test Loss: 2.0795\n",
      "Test Accuracy: 0.6447\n",
      "----------------------------------------\n",
      "Epoch 10/10\n",
      "Train Loss: 0.0014\n",
      "Test Loss: 2.1354\n",
      "Test Accuracy: 0.6457\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train_loss = train(model, train_loader, optimizer, scheduler, device)\n",
    "    \n",
    "    test_loss, test_accuracy = evaluate(model, test_loader, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('10_epochs_model_deberta/tokenizer_config.json',\n",
       " '10_epochs_model_deberta/special_tokens_map.json',\n",
       " '10_epochs_model_deberta/tokenizer.json')"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('10_epochs_model_deberta')\n",
    "tokenizer.save_pretrained('10_epochs_model_deberta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normal_tags(tags):\n",
    "#     new_tags = []\n",
    "#     for tag in tags:\n",
    "#         for term_type in term_types:\n",
    "#             if term_type in tag:\n",
    "#                 new_tags.append(tag)\n",
    "# all_data = []\n",
    "# for _ in range(4000):\n",
    "#     client = arxiv.Client()\n",
    "#     search = arxiv.Search(query = \"submittedDate:[201901010600 TO 202501010600]\", max_results=100)\n",
    "#     all_data = []\n",
    "#     for r in client.results(search):\n",
    "#         try:\n",
    "#             if r:\n",
    "#                 summary = r.summary\n",
    "#                 tag = normal_tags(r.categories)\n",
    "#                 title = r.title\n",
    "#                 if title and summary and tag:\n",
    "#                     data = {\n",
    "#                         'summary': summary,\n",
    "#                         'tag': tag,\n",
    "#                         'title': title\n",
    "#                     }\n",
    "#                     all_data.append(data)\n",
    "#         except Exception:\n",
    "#             continue\n",
    "\n",
    "\n",
    "# df_new = pd.DataFrame(all_data)\n",
    "# df_new.to_csv('new_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Код приложения\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "\n",
    "@st.cache_resource  # кэширование\n",
    "def load_model():\n",
    "    return AutoModelForSequenceClassification.from_pretrained('YSDA/2-ML/HF_tune_Transformers/10_epochs_model_deberta')\n",
    "\n",
    "@st.cache_resource  # кэширование\n",
    "def load_tokenizer():\n",
    "    return AutoTokenizer.from_pretrained('YSDA/2-ML/HF_tune_Transformers/10_epochs_model_deberta')\n",
    "\n",
    "@st.cache_resource  # кэширование\n",
    "def load_dict():\n",
    "    with open('YSDA/2-ML/HF_tune_Transformers/dictionary.json', 'r', encoding='utf-8') as file:\n",
    "        label_dict = json.load(file)\n",
    "    return label_dict\n",
    "\n",
    "model = load_model()\n",
    "tokenizer = load_tokenizer()\n",
    "label_dict = load_dict()\n",
    "\n",
    "st.title(\"Классификатор статей\")\n",
    "st.markdown(\"Данное веб-приложение может предсказать тематику статьи на основе ее названия и описания.\\nЧтобы воспользоваться функциональностью, просто введите название и описание в поля ниже.\")\n",
    "\n",
    "query1 = st.text_input(\"Название статьи\")\n",
    "query2 = st.text_input(\"Описание статьи\")\n",
    "\n",
    "if query1 and query2:\n",
    "    query = f'''\n",
    "    Input: {query1}\n",
    "    Summary: {query2}\n",
    "    '''\n",
    "    \n",
    "    inputs = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    \n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    \n",
    "    prediction = np.argmax(logits, axis=1)\n",
    "    \n",
    "    probabilities = torch.softmax(torch.from_numpy(logits), dim=1).numpy()\n",
    "    \n",
    "    top_n = 5\n",
    "    top_predictions = [label_dict[str(i)] for i in np.argsort(probabilities, axis=1)[:, -top_n:][:, ::-1][0]]\n",
    "    top_probabilities = np.sort(probabilities, axis=1)[:, -top_n:][:, ::-1]\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Тематика': top_predictions,\n",
    "        'Вероятность': top_probabilities[0]\n",
    "    })\n",
    "\n",
    "    st.dataframe(df)\n",
    "    on = st.toggle(\"Показать смешную картинку\")\n",
    "    if on:\n",
    "        st.markdown(\"Для хорошего настроения проверяющего\")\n",
    "        st.image(\"YSDA/2-ML/HF_tune_Transformers/image.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
